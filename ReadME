üß† Employee Attrition Prediction using AWS SageMaker
This project demonstrates how to build, train, evaluate, and deploy a machine learning model on AWS SageMaker to predict employee attrition. It uses Logistic Regression to classify whether an employee is likely to leave the organization based on demographic and employment-related features.
üìã Project Overview
The main objective of this project is to use AWS SageMaker along with Python libraries like boto3, pandas, scikit-learn, and seaborn to:
Connect to AWS resources such as S3.
Retrieve and preprocess employee data.
Train a Logistic Regression model for attrition prediction.
Evaluate the model using key performance metrics.
Deploy and store the trained model in an AWS S3 bucket for future predictions.
üß© Tasks Breakdown
Task 1: Setting Up AWS Resources
Established an AWS session using boto3.
Retrieved the SageMaker execution role ARN for authentication.
Connected to an S3 bucket named employeedataset12 and downloaded the dataset Employee.csv.
Loaded the dataset into a pandas DataFrame for analysis.
Task 2 & 3: Data Exploration and Visualization
Verified and removed duplicate records.
Analyzed data shape and structure.
Created visualizations:
A pie chart showing the gender distribution.
A count plot displaying the education-level distribution.
These visual insights help understand the demographic balance of the workforce.
Task 4: Feature Engineering
Split the dataset into features (X) and target variable (Y).
Applied OneHotEncoding to categorical variables and passthrough transformation for numerical features using ColumnTransformer.
Ensured the feature matrix was ready for model training.
Task 5: Model Building and Evaluation
Split the dataset into training (80%) and testing (20%) sets.
Scaled the features using StandardScaler to normalize values.
Built a Logistic Regression model to predict employee attrition.
Evaluated the model using:
Accuracy
Precision
Recall
F1 Score
Generated a confusion matrix heatmap to visualize true vs. predicted classifications.
Final Model Performance:
Metric	Score
Accuracy	0.72
Precision	0.63
Recall	0.42
F1-Score	0.50
Task 6: Model Deployment
Serialized the trained Logistic Regression model using joblib.
Uploaded the serialized model to AWS S3 in the path:
s3://employeedataset12/models/logreg_model.pkl
Ensured successful deployment by verifying S3 upload completion.
Task 7: Model Prediction from S3
Downloaded the serialized model from S3 using boto3 and tempfile.
Deserialized it back into memory for inference.
Re-tested the model using test data to confirm prediction consistency.
Verified the same accuracy and F1-score after reloading, confirming the model‚Äôs portability.
üß† Key Learnings
Integration of AWS SDK (boto3) with SageMaker for ML workflows.
Data engineering and preprocessing using scikit-learn pipelines.
Model evaluation through standard metrics and visualization techniques.
Model serialization and deployment to AWS S3 for scalable use.
Real-world application of end-to-end ML pipeline automation.
üß∞ Tech Stack
Category	Tools & Services
Cloud Platform	AWS SageMaker, AWS S3
Programming Language	Python
Libraries	pandas, numpy, scikit-learn, seaborn, matplotlib, joblib, boto3
Model	Logistic Regression
Visualization	Seaborn, Matplotlib
‚öôÔ∏è Workflow Summary
Data Ingestion ‚Äì Load data from AWS S3.
Data Cleaning ‚Äì Remove duplicates and inconsistencies.
Feature Engineering ‚Äì Encode categorical variables and scale numeric ones.
Model Training ‚Äì Train Logistic Regression on processed data.
Evaluation ‚Äì Measure model performance via metrics and confusion matrix.
Deployment ‚Äì Serialize and push model to S3.
Inference ‚Äì Reload model from S3 for predictions.
üì¶ Output
Trained Logistic Regression model file:
logreg_model.pkl stored in s3://employeedataset12/models/
Confusion matrix and visual charts illustrating insights.
Performance metrics ensuring reliability for deployment.
üîí Data Privacy & Security
AWS IAM roles were used for secure access.
No sensitive or personally identifiable data is exposed.
Temporary credentials and secure API handling ensure compliance with best practices.
üìà Future Enhancements
Replace Logistic Regression with advanced models like XGBoost or Random Forest.
Automate model retraining via SageMaker Pipelines.
Integrate real-time predictions using SageMaker Endpoints.
Add dashboard visualization for HR analytics insights.
üë®‚Äçüíª Author
Arvind Sathyan
Data & AI Engineer | AWS Certified | Machine Learning Practitioner
üìß Contact: [Your Email or LinkedIn]
